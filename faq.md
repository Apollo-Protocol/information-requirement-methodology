# Frequently Asked Questions

##	But I already have a lot of data. Can use I it to take short-cuts?

No.  However, you can use it in your gap analysis.  It is vital that you do analyse data that is already in use until Stage 3.  Knowing how closely (or not) it is to the required quality is important to understanding but doing this too early can compromise objectivity in the first two stages.

##	This looks like a lot; how do I get started?

Section 2 explains that the only requirement to get started is a recognition that there is a need to address data quality and information management through a process of continuous improvement.  It also helps to have sufficient support in an organisation looking to adopt the approach, an enthusiasm to learn and some areas to address (i.e. and initial scope).

##	Where can I get help?

Documentation and other supporting material will be produced as the IMF is developed.  The [Apollo Protocol](https://digitaltwinhub.co.uk/networks/29-the-apollo-protocol/) is supporting this space so please get in touch to ask questions and to find communities that are on the same journey.

## Can I get involved in pilot activities or the development of the RDL?

Having a go is the best way to learn.  You don’t need to wait but there will be opportunities to help community development and allow sharing of developing expertise and experience.

##	I have invested a lot in my current information systems, do I need to start again?

No, although it can initially appear that way.  Taking an objective look at the data quality requirements throughout the lifecycle of activities in any area of application can seem like starting again but that is often because it has never been done in the first place.  Once the requirements are known it can be easy to identify quality shortcomings in existing systems and the improvements required, Stage 3.  They can then be planned for naturally as part of the approach of continuous improvement.  This is part of Stage 4.

##	Are there any tools, products or services available to help?

A key message is that it is the commitment to improving data quality and the application of the methodology that is the primary goal of this document.  These documents illustrate the approach to capturing information requirements and supporting its adoption.  This should result in tools being created and adapted as progress is made. An early prototype has been developed [here](https://github.com/Apollo-Protocol/4d-activity-editor).

##	It looks like I need to know the future to execute this methodology

No, this is the wrong way to think about it.  Although we can’t know everything in advance we can know much of what will be needed.  The better we can do at identifying what is, and is likely to be, needed the better we’ll be at delivering outcomes through the use of information.  Quality management is about being committed to improving through continuous improvement and this methodology shows how it can be done for data quality.

## Why have I never seen this done before. Why is this?

Data has been commoditised over a few decades as computers, storage and communications technology has encouraged its growth.  The cost has been a reduction in attention to the quality of data.  Despite this we expect information systems to work increasingly well.  Without addressing data quality there is a plateau to the benefits that can be obtained from information systems that we have arguably reached.

##	I have seen recent standards and industry bodies calling for information requirements to be addressed in detail. Does this methodology fit with that?

Yes.  This methodology directly addresses this.  In recent years documents have increasingly referenced addressing information requirements at any required level of detail.  Doing this is generally left as an exercise for organisations to discover for themselves.  Documents like this provide a way to address this need.

##	Can I not just map data between systems at their interfaces?

This is addressed in (3).  Point-to-point mapping between systems is an expensive approach to making systems work and is not a scalable solution to addressing data integration.  The methodology has been created to enable the consistency required across information systems to meet any, and all, identified data quality requirements.
